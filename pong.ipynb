{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('py379': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3de95b49fbd3a3beaf1c6a45f5cbe1d182ff22a2879d5e3a6f5a6dacfdbdfaaa"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Pong RL DQN implementation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Teaching a machine to play an Atari game PONG using reinforcement learning, code adapted from TF-Agents tutorials and (Geron 2019).\n",
    "\n",
    "Specific libraries:\n",
    "\n",
    "    - tensorflow, keras\n",
    "    - gym Atari environment (pip install gym[atari])\n",
    "\n",
    "References:\n",
    "\n",
    "    - Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Riedmiller, M. A. (2013). Playing atari with deep reinforcement learning CoRR, abs/1312.5602. Retrieved from http://arxiv.org/abs/1312.5602\n",
    "    - Geron, A. (2019). Hands-on machine learning with scikit-learn, keras, and tensorflow: Concepts, tools, and techniques to build intelligent systems (2nd ed.). O’Reilly UK Ltd.\n",
    "    - Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction (2nd ed.). Cambridge: MIT Press.\n",
    "    - Lillicrap, T., P., Hunt, J., Pritzel, A.,Heess, N., Erez, T., Tassa, Y., Silver, D., Wierstra, D. (2016 ) Continuous control with deep reinforcement learning. Retrieved from http://arxiv.org/abs/1509.02971 \n",
    "\n",
    "    \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ImportError",
     "evalue": "libpython3.7m.so.1.0: cannot open shared object file: No such file or directory",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-72b0f5d544fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimeLimit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mreverb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# import required tf-agents components\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py379/lib/python3.7/site-packages/reverb/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# pylint: enable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mreverb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitem_selectors\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mselectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mreverb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrate_limiters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py379/lib/python3.7/site-packages/reverb/item_selectors.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mreverb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpybind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mFifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpybind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFifoSelector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py379/lib/python3.7/site-packages/reverb/pybind.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_tf\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlibpybind\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0m_tf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: libpython3.7m.so.1.0: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "## import basic libraries:\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib as mpl\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import logging\n",
    "#import pyvirtualdisplay\n",
    "from functools import partial\n",
    "from gym.wrappers import TimeLimit\n",
    "import time\n",
    "import reverb\n",
    "\n",
    "# import required tf-agents components\n",
    "from tf_agents.environments import suite_gym, suite_atari\n",
    "from tf_agents.environments.atari_preprocessing import AtariPreprocessing\n",
    "from tf_agents.environments.atari_wrappers import FrameStack4\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "from tf_agents.eval.metric_utils import log_metrics\n",
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "from tf_agents.trajectories.trajectory import to_transition\n",
    "from tf_agents.utils.common import function\n",
    "#from tf_agents.specs import array_spec\n",
    "#from tf_agents.specs import tensor_spec\n",
    "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
    "\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stable and repeatable runs:\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# load the atari environment\n",
    "max_episode_frames = 108000 \n",
    "env_name = \"PongNoFrameskip-v4\"\n",
    "\n",
    "env = suite_atari.load(\n",
    "    env_name,\n",
    "    max_episode_steps=max_episode_frames/4,\n",
    "    gym_env_wrappers=[AtariPreprocessing, FrameStack4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=160x210 at 0x7F2E284BA990>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAADSCAIAAABCR1ywAAACh0lEQVR4nO3bPUoDURhAUSMD2lu4CDdgaZeVWNq6GVeSzjIbcBEp0sfOJsg0EgPODLmeUwXyMw8uHw/mTVZXP3h5ffjpLS7Ias6Qz0+nr/X2/jHDSuZx+Nyc/MztzXrSNVxP+ussTuA4geOGpS483mt/szdfuvFe+5u9+a+Y4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOW+xe9H+4/zw25/3nMRMcJ3CcwHGzPpPF/ExwnMBxw3a3X3oNTMgExwkcJ3CcwHECxwkcJ3CcwHECxw2P93dLr4EJmeA4geMEjhM4TuA4geMEjhM4TuA4geOOgbe7vafvkkxwnMBxAscd/x/s0LDKBMcJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHDcsvQBOO3xuvl/f3qzP+q4JjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA458EX4Nwz4DETHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHDdsd/ul18CETHCcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECx30Bu1ci6JJ73gAAAAAASUVORK5CYII=\n"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "env.reset()\n",
    "PIL.Image.fromarray(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 4 time steps, each taking a action step left\n",
    "env.reset()\n",
    "time_step = env.step(1) # FIRE\n",
    "for _ in range(4):\n",
    "    time_step = env.step(3) # LEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from (Geron, 2017) - plot 4 steps in one observation utilising RGB color channels\n",
    "def plot_observation(obs):\n",
    "    obs = obs.astype(np.float32)\n",
    "    img = obs[..., :3]\n",
    "    current_frame_delta = np.maximum(obs[..., 3] - obs[..., :3].mean(axis=-1), 0.)\n",
    "    img[..., 0] += current_frame_delta\n",
    "    img[..., 2] += current_frame_delta\n",
    "    img = np.clip(img / 150, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 288x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"231.84pt\" version=\"1.1\" viewBox=\"0 0 231.84 231.84\" width=\"231.84pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-01-09T21:56:31.741950</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 231.84 \nL 231.84 231.84 \nL 231.84 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g clip-path=\"url(#pec68bf3a74)\">\n    <image height=\"218\" id=\"image5e7374f20f\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"7.2\" xlink:href=\"data:image/png;base64,\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\" y=\"-6.64\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pec68bf3a74\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"7.2\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plot_observation(time_step.observation)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make the environment usable from within TF graph\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "tf_env = TFPyEnvironment(env)"
   ]
  },
  {
   "source": [
    "## DQN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Network, DQN\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "\n",
    "preprocessing_layer = keras.layers.Lambda(lambda obs: tf.cast(obs, np.float32) / 255.)\n",
    "conv_layer_params=[(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)]\n",
    "fc_layer_params=[512]\n",
    "\n",
    "q_net = QNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    preprocessing_layers=preprocessing_layer,\n",
    "    conv_layer_params=conv_layer_params,\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "source": [
    "## Agent"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent\n",
    "\n",
    "#allowlr =array_spec.BoundedArraySpec(shape=(), dtype=np.int64, name='action', minimum=0, maximum=6)\n",
    "\n",
    "train_step = tf.Variable(0)\n",
    "update_period = 4 # run a training step every 4 collect steps\n",
    "learning_rate = 2.5e-4\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "epsilon_fn = keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=1.0, # initial ε\n",
    "    decay_steps=150000 // update_period, # <=> 1,000,000 ALE frames\n",
    "    end_learning_rate=0.01) # final ε\n",
    "\n",
    "agent = DqnAgent(tf_env.time_step_spec(),\n",
    "                 #allowlr,\n",
    "                 tf_env.action_spec(),\n",
    "                 q_network=q_net,\n",
    "                 optimizer=optimizer,\n",
    "                 target_update_period=2000, # <=> 32,000 ALE frames\n",
    "                 td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "                 gamma=0.99, # discount factor\n",
    "                 train_step_counter=train_step,\n",
    "                 epsilon_greedy=lambda: epsilon_fn(train_step))\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show progress class\n",
    "\n",
    "class ShowProgress:\n",
    "    def __init__(self, total):\n",
    "        self.counter = 0\n",
    "        self.total = total\n",
    "    def __call__(self, trajectory):\n",
    "        if not trajectory.is_boundary():\n",
    "            self.counter += 1\n",
    "        if self.counter % 100 == 0:\n",
    "            print(\"\\r{}/{}\".format(self.counter, self.total), end=\"\")"
   ]
  },
  {
   "source": [
    "## Replay Buffer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "replay_buffer_max_length = 5000\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "replay_buffer_observer = replay_buffer.add_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From /home/riku/miniconda3/envs/py379/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py:1218: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    }
   ],
   "source": [
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=64,\n",
    "    num_steps=2,\n",
    "    num_parallel_calls=3).prefetch(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'reverb' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2cfadb52f87b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrate_limiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreverb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrate_limiters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSampleToInsertRatio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples_per_insert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_size_to_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_buffer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'reverb' is not defined"
     ]
    }
   ],
   "source": [
    "rate_limiter=reverb.rate_limiters.SampleToInsertRatio(samples_per_insert=3.0, min_size_to_sample=3, error_buffer=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'uniform_table'\n",
    "table = reverb.Table(\n",
    "    table_name,\n",
    "    max_size=replay_buffer_capacity,\n",
    "    sampler=reverb.selectors.Uniform(),\n",
    "    remover=reverb.selectors.Fifo(),\n",
    "    rate_limiter=reverb.rate_limiters.MinSize(1))\n",
    "\n",
    "reverb_server = reverb.Server([table])"
   ]
  },
  {
   "source": [
    "## Logging"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training metrics\n",
    "\n",
    "train_metrics = [\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    tf_metrics.EnvironmentSteps(),\n",
    "    tf_metrics.AverageReturnMetric(),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logging.basicConfig(filename='DQN.log', level=logging.INFO)\n",
    "log_metrics(train_metrics)"
   ]
  },
  {
   "source": [
    "## Collect Driver"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the Collect Driver, passes current tie step to the collect policy, which returns the action step\n",
    "\n",
    "collect_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    agent.collect_policy,\n",
    "    observers=[replay_buffer_observer] + train_metrics,\n",
    "    num_steps=update_period) # collect 4 steps for each training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From /home/riku/miniconda3/envs/py379/lib/python3.7/site-packages/tf_agents/drivers/dynamic_step_driver.py:203: calling while_loop_v2 (from tensorflow.python.ops.control_flow_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.while_loop(c, b, vars, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.while_loop(c, b, vars))\n",
      "20000/20000"
     ]
    }
   ],
   "source": [
    "# warm up the replay buffer with random policy\n",
    "initial_collect_policy = RandomTFPolicy(tf_env.time_step_spec(),\n",
    "                                        tf_env.action_spec())\n",
    "init_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    initial_collect_policy,\n",
    "    observers=[replay_buffer.add_batch, ShowProgress(20000)],\n",
    "    num_steps=20000) # <=> 80,000 ALE frames as in 2015 DQN paper by DeepMind\n",
    "final_time_step, final_policy_state = init_driver.run()"
   ]
  },
  {
   "source": [
    "## Create the Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Training Loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the main functions to TF functions\n",
    "\n",
    "collect_driver.run = function(collect_driver.run)\n",
    "agent.train = function(agent.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(n_iterations):\n",
    "    time_step = None\n",
    "    policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)\n",
    "    iterator = iter(dataset)\n",
    "    startt=time.time()\n",
    "    for iteration in range(n_iterations):\n",
    "        time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
    "        trajectories, buffer_info = next(iterator)\n",
    "        train_loss = agent.train(trajectories)\n",
    "        print(\"\\r{} loss:{:.5f}\".format(iteration, train_loss.loss.numpy()), end=\"\")\n",
    "        if iteration % 500 == 0:\n",
    "            logg=[]\n",
    "            for a in train_metrics:\n",
    "                logg = np.round(np.append(logg, np.asscalar(a.result().numpy())),2)\n",
    "            f=open('DQN.txt','ab')\n",
    "            np.savetxt(f, [logg], delimiter=\",\", fmt='%.1f')\n",
    "            f.close()\n",
    "            startt=time.time()\n",
    "            print(np.round(logg,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 loss:0.02971[   9.  8192.   -19.7  893.1]\n",
      "129 loss:0.00087"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-77413613285a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-65-0c32766c9a8f>\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m(n_iterations)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_driver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtrajectories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\r{} loss:{:.5f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py379/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py379/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py379/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py379/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py379/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/miniconda3/envs/py379/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_agent(n_iterations=200000)"
   ]
  },
  {
   "source": [
    "## Create visualisation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1000/1000"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.animation.FuncAnimation at 0x7f207cfa3b10>"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "frames = []\n",
    "def save_frames(trajectory):\n",
    "    global frames\n",
    "    frames.append(tf_env.pyenv.envs[0].render(mode=\"rgb_array\"))\n",
    "\n",
    "prev_lives = tf_env.pyenv.envs[0].ale.lives()\n",
    "def reset_and_fire_on_life_lost(trajectory):\n",
    "    global prev_lives\n",
    "    lives = tf_env.pyenv.envs[0].ale.lives()\n",
    "    if prev_lives != lives:\n",
    "        tf_env.reset()\n",
    "        tf_env.pyenv.envs[0].step(1)\n",
    "        prev_lives = lives\n",
    "\n",
    "watch_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    agent.policy,\n",
    "    observers=[save_frames, reset_and_fire_on_life_lost, ShowProgress(1000)],\n",
    "    num_steps=1000)\n",
    "final_time_step, final_policy_state = watch_driver.run()\n",
    "\n",
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(\"pong.gif\")\n",
    "frame_images = [PIL.Image.fromarray(frame) for frame in frames[:500]]\n",
    "frame_images[0].save(image_path, format='GIF',\n",
    "                     append_images=frame_images[1:],\n",
    "                     save_all=True,\n",
    "                     duration=60,\n",
    "                     loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<img src=\"images/rl/pong.gif\" />\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"pong.gif\" />"
   ]
  }
 ]
}
